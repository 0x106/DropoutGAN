Initial work on using dropout in a principled fashion to increase the capacity
of a neural network without (1) increasing memory.


(1) It uses a separate autoencoder, but I show that we can increase the performance
of an arbitrary classifier without changing its model.
